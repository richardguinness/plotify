{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "732829f7-dacb-4dbe-9478-68c10eb5e49c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "732829f7-dacb-4dbe-9478-68c10eb5e49c",
        "outputId": "205ed70d-16ae-4512-bfb0-7cb0e0ad736a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformers                  4.26.0.dev0\n"
          ]
        }
      ],
      "source": [
        "!pip list | grep transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "43bab354-e254-44ac-8793-999f4498b7d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43bab354-e254-44ac-8793-999f4498b7d4",
        "outputId": "8a09449c-9887-42f3-ab82-df49ccfa8603"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasets                      2.8.0\n",
            "tensorflow-datasets           4.8.1\n",
            "vega-datasets                 0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip list | grep dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upgrade to latest version from main as per\n",
        "# https://github.com/huggingface/transformers/issues/20750\n",
        "# to avoid\n",
        "# AttributeError: module 'keras.engine.data_adapter' has no attribute 'expand_1d'\n",
        "# when fitting\n",
        "!pip install --upgrade git+https://github.com/huggingface/transformers.git\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "3fP9Xh-C6Thi"
      },
      "id": "3fP9Xh-C6Thi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "426ffdb1-55d1-4275-a986-52494f557287",
      "metadata": {
        "id": "426ffdb1-55d1-4275-a986-52494f557287"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "dc3b28be-8123-4337-824a-b295968e7403",
      "metadata": {
        "id": "dc3b28be-8123-4337-824a-b295968e7403"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# from transformers import TFGPT2LMHeadModel\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel, pipeline, AutoTokenizer, TFAutoModelForCausalLM\n",
        "from datasets import Dataset, load_dataset\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d360b762-a0bc-425e-bba4-a3ae612d54b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d360b762-a0bc-425e-bba4-a3ae612d54b1",
        "outputId": "178ca83c-a24a-4491-ecb5-0c981ec02bc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REPLICAS:  1\n"
          ]
        }
      ],
      "source": [
        "# Try to run on TPU if available\n",
        "# Detect hardware, return appropriate distribution strategy\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    print(\"Running on TPU \", tpu.cluster_spec().as_dict()[\"worker\"])\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS = 128\n",
        "\n",
        "EOS_TOKEN = \"<|endoftext|>\"\n",
        "PAD_TOKEN = \"<|pad|>\""
      ],
      "metadata": {
        "id": "2jGce1GJNh68"
      },
      "id": "2jGce1GJNh68",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "965ad243-2895-4b85-b913-589b8e87ce7c",
      "metadata": {
        "id": "965ad243-2895-4b85-b913-589b8e87ce7c"
      },
      "outputs": [],
      "source": [
        "# ORIGINAL\n",
        "\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained(\n",
        "#     'gpt2',\n",
        "#     eos_token=EOS_TOKEN,\n",
        "#     pad_token=PAD_TOKEN,\n",
        "#     max_length=MAX_TOKENS,\n",
        "#     padding_side='left'\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    'gpt2',\n",
        "    # eos_token=EOS_TOKEN,\n",
        "    pad_token='</s>',\n",
        "    max_length=MAX_TOKENS,\n",
        "    padding_side='left'\n",
        ")\n",
        "\n",
        "# tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "dOT6QLo5OGlQ",
        "outputId": "bebad46c-9d7e-4d2f-9c15-b622a49cb662",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "dOT6QLo5OGlQ",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f1549414-31ce-4a9f-84b5-c13355669c71",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1549414-31ce-4a9f-84b5-c13355669c71",
        "outputId": "2d4d5ee6-028f-48bf-8672-c12b208a6cac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-7d45206e1c87ab3a\n",
            "/usr/local/lib/python3.8/dist-packages/datasets/builder.py:712: FutureWarning: 'use_auth_token' was deprecated in version 2.7.1 and will be removed in 3.0.0. Pass `use_auth_token` to the initializer/`load_dataset_builder` instead.\n",
            "  warnings.warn(\n",
            "WARNING:datasets.builder:Found cached dataset text (/root/.cache/huggingface/datasets/text/default-7d45206e1c87ab3a/0.0.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' Living on Mars , Deckard is acting as a consultant to a movie crew filming the story of his Blade Runner days . He finds himself drawn into a mission on behalf of the replicants he was once assigned to kill . Meanwhile , the mystery surrounding the beginnings of the Tyrell Corporation is being dragged out into the light . ',\n",
              " ' Beginning several months after the events in Blade Runner , Deckard has retired to an isolated shack outside the city , taking the replicant Rachael with him in a Tyrell transport container , which slows down the replicant aging process . He is approached by a woman who explains she is Sarah Tyrell , niece of Eldon Tyrell , heiress to the entire Tyrell Corporation and the human template templant for the Rachael replicant . She asks Deckard to hunt down the missing sixth replicant . At the same time , the human template for Roy Batty hires Dave Holden , the blade runner attacked by Leon , to help him hunt down the man he believes is the sixth replicant Deckard . Deckard and Holden s investigations lead them to re visit Sebastian , Bryant , and John Isidore from the book Do Androids Dream Of Electric Sheep ? , learning more about the nature of the blade runners and the replicants . When Deckard , Batty , and Holden finally clash , Batty s inhuman fighting prowess leads Holden to believe he has been duped all along and that Batty is the sixth replicant he shoots him . Deckard returns to Sarah with his suspicion there is no sixth replicant . Sarah , speaking via a remote camera , confesses that she created and maintained the rumor herself , to deliberately discredit and eventually destroy the Tyrell Corporation , after her uncle Eldon created Rachael based on her and then abandoned the real Sarah . Sarah brings Rachael back to the Corporation building to meet with Deckard , and he escapes with her . However , Holden recovering from his injuries during the fight later finds the truth Rachael has been killed by Tyrell agents , and the Rachael who escaped with Deckard was actually Sarah . She has completed her revenge by both destroying Tyrell , and taking back Rachael s place . ']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "ds = Dataset.from_text('booksummaries_cleaned2.txt')# 'testdata2.txt') # 'text100.txt') # 'booksummaries_cleaned2.txt')\n",
        "ds['text'][:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d3bcb95-bbff-462a-9dba-242d4bf27116",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "8d3bcb95-bbff-462a-9dba-242d4bf27116"
      },
      "source": [
        "## Explanation of working of tokenize_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "86c09e6a-19de-416a-b99e-952a216894cb",
      "metadata": {
        "id": "86c09e6a-19de-416a-b99e-952a216894cb"
      },
      "outputs": [],
      "source": [
        "# examples = [BOS_TOKEN + ex + EOS_TOKEN for ex in ds[\"text\"]]\n",
        "# examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9d20f85b-f042-4077-ad3c-55f6981feff3",
      "metadata": {
        "id": "9d20f85b-f042-4077-ad3c-55f6981feff3"
      },
      "outputs": [],
      "source": [
        "# output = tokenizer(\n",
        "#     examples,\n",
        "#     add_special_tokens=True,  # Only adds pad not eos and bos\n",
        "#     max_length=MAX_TOKENS,\n",
        "#     truncation=True,\n",
        "#     padding='max_length',\n",
        "#     # return_tensors='tf'\n",
        "# )\n",
        "# output[\"input_ids\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3024dd33-a83f-4293-b1ca-05082641bc92",
      "metadata": {
        "id": "3024dd33-a83f-4293-b1ca-05082641bc92"
      },
      "outputs": [],
      "source": [
        "# # Drop the first token\n",
        "# output[\"labels\"] = [x[1:] for x in output[\"input_ids\"]]\n",
        "# pprint(output[\"labels\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "384bb5a1-c86d-4d9a-814c-9c406cfc9cde",
      "metadata": {
        "id": "384bb5a1-c86d-4d9a-814c-9c406cfc9cde"
      },
      "outputs": [],
      "source": [
        "# pprint(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ebaa495d-0233-47f6-8bfa-8565e111b8a3",
      "metadata": {
        "id": "ebaa495d-0233-47f6-8bfa-8565e111b8a3"
      },
      "outputs": [],
      "source": [
        "# # Replace all occurences of PAD_TOKEN with -100\n",
        "# output[\"labels\"] = [\n",
        "#     [-100 if x == tokenizer.pad_token_id else x for x in y] for y in output[\"labels\"]\n",
        "# ]\n",
        "# pprint(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f2d207f5-e098-4c18-a2b3-61721ba4f88f",
      "metadata": {
        "id": "f2d207f5-e098-4c18-a2b3-61721ba4f88f"
      },
      "outputs": [],
      "source": [
        "# # truncate input ids and attention mask to account for label shift\n",
        "# output[\"input_ids\"] = [x[:-1] for x in output[\"input_ids\"]]\n",
        "# output[\"attention_mask\"] = [x[:-1] for x in output[\"attention_mask\"]]\n",
        "# pprint(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25c06772-572d-44a9-b34a-2d09e659f116",
      "metadata": {
        "id": "25c06772-572d-44a9-b34a-2d09e659f116"
      },
      "source": [
        "## tokenize_function definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "db6a8808-8199-405f-bd4f-e499fa48a112",
      "metadata": {
        "id": "db6a8808-8199-405f-bd4f-e499fa48a112"
      },
      "outputs": [],
      "source": [
        "# Consolidate into a function\n",
        "\n",
        "def tokenize_function(examples, tokenizer=tokenizer):\n",
        "\n",
        "    examples = [ex + EOS_TOKEN for ex in examples[\"text\"]]\n",
        "\n",
        "    output = tokenizer(\n",
        "        examples,\n",
        "        add_special_tokens=True,  # Only adds pad not eos and bos\n",
        "        max_length=MAX_TOKENS,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        # return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    # Drop the first token\n",
        "    output[\"labels\"] = [x[1:] for x in output[\"input_ids\"]]\n",
        "\n",
        "    # Replace all occurences of PAD_TOKEN with -100\n",
        "    output[\"labels\"] = [\n",
        "        [-100 if x == tokenizer.pad_token_id else x for x in y] for y in output[\"labels\"]\n",
        "    ]\n",
        "\n",
        "    # truncate input ids and attention mask to account for label shift\n",
        "    output[\"input_ids\"] = [x[:-1] for x in output[\"input_ids\"]]\n",
        "    output[\"attention_mask\"] = [x[:-1] for x in output[\"attention_mask\"]]\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d38759fa-c138-4b34-9812-6f1f8ac3cb88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122,
          "referenced_widgets": [
            "e43bcc64a96f44a0bb561c0d95a32d42",
            "e80ef4334a134e0ebf6290a63d292032",
            "b6fb76adb08e4627a50cf05692d984a5",
            "847e84f7ea414eb188e7f7f3c39d6da0",
            "8b1bb191b50d451da2d0dc8214fea25b",
            "24a330a54c7b4be59eb958d202c72a73",
            "1d12658cbc024ab8aca97946ae7f3d51",
            "dce508846ac143f7a8b65629f60dc0f5",
            "d580a52c476f4c92871087559e268dc7",
            "d55443d43cf44113a85d53106e9ea13f",
            "c18aa6d09d234533b9fff6fc6d7a0a05"
          ]
        },
        "id": "d38759fa-c138-4b34-9812-6f1f8ac3cb88",
        "outputId": "f26e77be-5666-4a45-8505-9644cea7fb9f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/12 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e43bcc64a96f44a0bb561c0d95a32d42"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 11783\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "ds_tokenized = ds.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\"],\n",
        "    load_from_cache_file=False,\n",
        ")\n",
        "ds_tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9c6c7cd3-1498-4250-a7ad-d84620a13e19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c6c7cd3-1498-4250-a7ad-d84620a13e19",
        "outputId": "657b6bfa-0172-4165-a0a9-eeb83696f5df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached split indices for dataset at /root/.cache/huggingface/datasets/text/default-7d45206e1c87ab3a/0.0.0/cache-6ed01908ae47b3d0.arrow and /root/.cache/huggingface/datasets/text/default-7d45206e1c87ab3a/0.0.0/cache-a63491887333d709.arrow\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 9426\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 2357\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "ds_tokenized.set_format(type=\"python\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "ds_tokenized = ds_tokenized.train_test_split(\n",
        "    test_size=0.20, shuffle=True, seed=1, load_from_cache_file=True\n",
        ")\n",
        "ds_tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "564c024e-357b-4605-a0fc-b4cd7de21b69",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "564c024e-357b-4605-a0fc-b4cd7de21b69",
        "outputId": "15031cb8-5bba-4be9-f203-e71b1ab8b47a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.45 s, sys: 409 ms, total: 3.86 s\n",
            "Wall time: 3.84 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# prepare for use in tensorflow\n",
        "train_tensor_inputs = tf.convert_to_tensor(ds_tokenized[\"train\"][\"input_ids\"])\n",
        "train_tensor_labels = tf.convert_to_tensor(ds_tokenized[\"train\"][\"labels\"])\n",
        "train_tensor_mask = tf.convert_to_tensor(ds_tokenized[\"train\"][\"attention_mask\"])\n",
        "train = tf.data.Dataset.from_tensor_slices(\n",
        "    (\n",
        "        {\"input_ids\": train_tensor_inputs, \"attention_mask\": train_tensor_mask},\n",
        "        # {\"labels\": train_tensor_labels},\n",
        "        train_tensor_labels,\n",
        "    )\n",
        ")\n",
        "\n",
        "test_tensor_inputs = tf.convert_to_tensor(ds_tokenized[\"test\"][\"input_ids\"])\n",
        "test_tensor_labels = tf.convert_to_tensor(ds_tokenized[\"test\"][\"labels\"])\n",
        "test_tensor_mask = tf.convert_to_tensor(ds_tokenized[\"test\"][\"attention_mask\"])\n",
        "test = tf.data.Dataset.from_tensor_slices(\n",
        "    (\n",
        "        {\"input_ids\": test_tensor_inputs, \"attention_mask\": test_tensor_mask},\n",
        "        # {\"labels\": test_tensor_labels},\n",
        "        test_tensor_labels,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert to a TF object using new method\n",
        "\n",
        "``` python\n",
        "ds_tf = ds_tokenized.to_tf_dataset(\n",
        "    columns=[\"input_ids\",\"attention_mask\"],\n",
        "    label_cols=[\"labels\"],\n",
        "    batch_size=1,\n",
        "    shuffle=True\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "V9nLB05Lmpx9"
      },
      "id": "V9nLB05Lmpx9"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "fbdb882f-b578-4561-93f7-85276cf0ac86",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbdb882f-b578-4561-93f7-85276cf0ac86",
        "outputId": "9c7d0f3f-34ba-4150-a1b8-e8b45ba1938f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BATCH_SIZE:  28\n",
            "len(tokenizer)  50258\n"
          ]
        }
      ],
      "source": [
        "# Model params\n",
        "BATCH_SIZE_PER_REPLICA = 28\n",
        "EPOCHS = 10\n",
        "INITAL_LEARNING_RATE = 0.001\n",
        "\n",
        "try:\n",
        "    BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
        "except NameError as e:\n",
        "    BATCH_SIZE = BATCH_SIZE_PER_REPLICA\n",
        "BUFFER_SIZE = len(train)\n",
        "\n",
        "print(\"BATCH_SIZE: \", BATCH_SIZE)\n",
        "\n",
        "print(\"len(tokenizer) \", len(tokenizer))\n",
        "\n",
        "# prepare data for consumption\n",
        "train_ds = (\n",
        "    train.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        ")\n",
        "test_ds = test.batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "af84fd8d-6b7c-4106-bce8-63c5b82acab3",
      "metadata": {
        "id": "af84fd8d-6b7c-4106-bce8-63c5b82acab3"
      },
      "outputs": [],
      "source": [
        "# Decreasing learning rate scheduler\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    INITAL_LEARNING_RATE,\n",
        "    decay_steps=500,\n",
        "    decay_rate=0.7,\n",
        "    staircase=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # ORIGINAL\n",
        "# # initialize model, use_cache=False important! else wrong shape at loss calc\n",
        "# with strategy.scope():\n",
        "#     model = TFGPT2LMHeadModel.from_pretrained(\n",
        "#         'gpt2',\n",
        "#         use_cache=False,\n",
        "#         pad_token_id=tokenizer.pad_token_id,\n",
        "#         eos_token_id=tokenizer.eos_token_id,\n",
        "#     )\n",
        "#     model.resize_token_embeddings(len(tokenizer))\n",
        "#     optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "#     model.compile(optimizer=optimizer, loss=model.hf_compute_loss)\n",
        "#     model.summary()"
      ],
      "metadata": {
        "id": "NXQ2YU6cO8eh"
      },
      "id": "NXQ2YU6cO8eh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize model, use_cache=False important! else wrong shape at loss calc\n",
        "with strategy.scope():\n",
        "    model = TFAutoModelForCausalLM.from_pretrained(\n",
        "        'gpt2',\n",
        "        use_cache=False,\n",
        "        pad_token_id=tokenizer.eos_token_id, # pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    # model.resize_token_embeddings(len(tokenizer))\n",
        "    # optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "    # model.compile(optimizer=optimizer, loss=model.hf_compute_loss)\n",
        "    model.summary()"
      ],
      "metadata": {
        "id": "AbmFiJrQO46_",
        "outputId": "977878cb-3e79-49ae-aaeb-02473721880a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "AbmFiJrQO46_",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tfgpt2lm_head_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " transformer (TFGPT2MainLaye  multiple                 124439808 \n",
            " r)                                                              \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 124,439,808\n",
            "Trainable params: 124,439,808\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1bce2c81-e38e-4765-8e41-ba97b5f2b0c4",
      "metadata": {
        "id": "1bce2c81-e38e-4765-8e41-ba97b5f2b0c4"
      },
      "outputs": [],
      "source": [
        "now = datetime.now().strftime(\"%Y-%m-%d_%H%M\")\n",
        "\n",
        "# # Fix TPU save model issue?\n",
        "# save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost') \n",
        "\n",
        "# Create callbacks\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping( \n",
        "        monitor=\"val_loss\", verbose=1, patience=1, restore_best_weights=True\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        \"models/\" + now + \"_GPT2-Model_{epoch:02d}_{val_loss:.4f}\",\n",
        "        monitor=\"val_loss\",\n",
        "        save_best_only=True,\n",
        "        verbose=1,\n",
        "        # options=save_locally, # Fix TPU save model issue?\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "ff48b183-b8d8-41da-9a2f-7bf3d90b68c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff48b183-b8d8-41da-9a2f-7bf3d90b68c9",
        "outputId": "ced51547-3aa1-491e-bfc4-1dabff4c86fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Params:\n",
            "batch_size: 28\n",
            "Epochs: 10\n",
            "Step p. Epoch: 336\n",
            "Initial Learning rate: 0.001\n",
            "-------------------------------------------------------------\n",
            "Epoch 1/10\n",
            "336/336 [==============================] - ETA: 0s - loss: 4.1995\n",
            "Epoch 1: val_loss improved from inf to 3.99660, saving model to models/2023-01-18_1417_GPT2-Model_01_3.9966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as wte_layer_call_fn, wte_layer_call_and_return_conditional_losses, dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, ln_f_layer_call_fn while saving (showing 5 of 294). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r336/336 [==============================] - 87s 223ms/step - loss: 4.1995 - val_loss: 3.9966\n",
            "Epoch 2/10\n",
            "336/336 [==============================] - ETA: 0s - loss: 3.5348\n",
            "Epoch 2: val_loss improved from 3.99660 to 3.99378, saving model to models/2023-01-18_1417_GPT2-Model_02_3.9938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as wte_layer_call_fn, wte_layer_call_and_return_conditional_losses, dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, ln_f_layer_call_fn while saving (showing 5 of 294). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r336/336 [==============================] - 70s 209ms/step - loss: 3.5348 - val_loss: 3.9938\n",
            "Epoch 3/10\n",
            "336/336 [==============================] - ETA: 0s - loss: 2.9349Restoring model weights from the end of the best epoch: 2.\n",
            "\n",
            "Epoch 3: val_loss did not improve from 3.99378\n",
            "336/336 [==============================] - 39s 116ms/step - loss: 2.9349 - val_loss: 4.1927\n",
            "Epoch 3: early stopping\n"
          ]
        }
      ],
      "source": [
        "# %%time    \n",
        "# Train Model\n",
        "steps_per_epoch = int(BUFFER_SIZE // BATCH_SIZE)\n",
        "print(\n",
        "    f\"Model Params:\\nbatch_size: {BATCH_SIZE}\\nEpochs: {EPOCHS}\\n\"\n",
        "    f\"Step p. Epoch: {steps_per_epoch}\\n\"\n",
        "    f\"Initial Learning rate: {INITAL_LEARNING_RATE}\"\n",
        "    \"\\n-------------------------------------------------------------\"\n",
        ")\n",
        "\n",
        "hist = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=test_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "3eb29058-358f-48b2-8254-cea6a3f0fc35",
      "metadata": {
        "id": "3eb29058-358f-48b2-8254-cea6a3f0fc35"
      },
      "outputs": [],
      "source": [
        "# path_to_model = '/content/models/2023-01-18_0754_GPT2-Model_02_3.9008'\n",
        "# # load model in tpu using Tensorflow's \"SavedModel\" format\n",
        "# with strategy.scope():\n",
        "#     # load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
        "#     restored_model = tf.keras.models.load_model(\n",
        "#         path_to_model,\n",
        "#         # options=load_locally,\n",
        "#         custom_objects={\"hf_compute_loss\": model.hf_compute_loss} # Important!\n",
        "#         )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# new_pipeline = pipeline(\n",
        "#             \"text-generation\",\n",
        "#             model=restored_model,\n",
        "#             tokenizer=tokenizer\n",
        "#         )"
      ],
      "metadata": {
        "id": "rj6CTl_HyL8_"
      },
      "id": "rj6CTl_HyL8_",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # We need to specify max_new_tokens here instead of max_length\n",
        "# new_pipeline('hello', max_new_tokens=10)"
      ],
      "metadata": {
        "id": "UhMD64ORv2gp"
      },
      "id": "UhMD64ORv2gp",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # HF save method\n",
        "# # Note: not using options=save_locally as per TPU fix\n",
        "# # implemented in callback & tf.keras.models.load_model()\n",
        "\n",
        "# model.save_pretrained('/content/models/hf', saved_model=True)\n"
      ],
      "metadata": {
        "id": "QkpoL_t-0If-"
      },
      "id": "QkpoL_t-0If-",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # HF load method\n",
        "# restored_model = TFGPT2LMHeadModel.from_pretrained('/content/models/hf')"
      ],
      "metadata": {
        "id": "q6WKAgzW2Vc7"
      },
      "id": "q6WKAgzW2Vc7",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_pipeline = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer\n",
        "        )"
      ],
      "metadata": {
        "id": "PEHXOqga9-gh"
      },
      "id": "PEHXOqga9-gh",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_pipeline('hello')#, max_new_tokens=10)"
      ],
      "metadata": {
        "id": "ye_dRZ25-C3r",
        "outputId": "2f424335-79b3-4db3-ef40-34a8feb85cf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ye_dRZ25-C3r",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/tf_utils.py:603: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/tf_utils.py:702: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 50 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'hello, a married man who lives in the city and falls in love with her for what she does. He is fascinated with the romance and begins fantasizing about the woman he meets his dreams, and eventually realizes that he has got her too. He'}]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing XLA\n",
        "\n",
        "Using these notes: https://huggingface.co/blog/tf-xla-generate"
      ],
      "metadata": {
        "id": "qFbH5HOIY26u"
      },
      "id": "qFbH5HOIY26u"
    },
    {
      "cell_type": "code",
      "source": [
        "# This is how we use the generate function directly, without using\n",
        "# a pipeline\n",
        "\n",
        "inputs = tokenizer([\"I went to the moon and \"], return_tensors=\"tf\")\n",
        "print(inputs)\n",
        "# generated = model.generate(**inputs, max_new_tokens=50)\n",
        "generated = model.generate(**inputs, do_sample=True, max_new_tokens=50)\n",
        "print(\"Sampling output: \", tokenizer.decode(generated[0]))"
      ],
      "metadata": {
        "id": "3hNOlnWU-E2L",
        "outputId": "6f92e2c6-eaf0-4f10-bdf8-4b1c41818f13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3hNOlnWU-E2L",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[  40, 1816,  284,  262, 8824,  290,  220]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/tf_utils.py:603: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampling output:  I went to the moon and  saw that my body was on fire and so I put my clothes and shoes back on. I couldn't think for a second on the next thing I went, I really couldn't remember if it was really my baby face or my baby figure.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a test function\n",
        "def most_likely_next_token(inputs):\n",
        "    model_output = model(inputs)\n",
        "    return tf.argmax(model_output.logits[:, -1, :], axis=-1)"
      ],
      "metadata": {
        "id": "LVtCjRBPXYSx"
      },
      "id": "LVtCjRBPXYSx",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print(\"Calling regular function with TensorFlow code...\")\n",
        "most_likely_next_token(inputs)"
      ],
      "metadata": {
        "id": "n8EyZ4qxZH-A",
        "outputId": "b312b5aa-ec3e-42dc-ca62-ed410a7b0d28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "n8EyZ4qxZH-A",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling regular function with TensorFlow code...\n",
            "CPU times: user 134 ms, sys: 5.12 ms, total: 139 ms\n",
            "Wall time: 138 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1,), dtype=int64, numpy=array([3711])>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xla_most_likely_next_token = tf.function(most_likely_next_token, jit_compile=True)"
      ],
      "metadata": {
        "id": "weKdAg2uX5A6"
      },
      "id": "weKdAg2uX5A6",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print(\"Calling XLA function... (for the first time -- will be slow)\")\n",
        "xla_most_likely_next_token(inputs)"
      ],
      "metadata": {
        "id": "bZuKIHQvYE80",
        "outputId": "f9715483-14d6-46bb-c463-a69bbe1f8ea1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "bZuKIHQvYE80",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling XLA function... (for the first time -- will be slow)\n",
            "CPU times: user 5 s, sys: 40.6 ms, total: 5.04 s\n",
            "Wall time: 6.43 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1,), dtype=int64, numpy=array([3711])>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print(\"Calling XLA function... (for the second time -- will be fast)\")\n",
        "xla_most_likely_next_token(inputs)"
      ],
      "metadata": {
        "id": "XUDJ9dnHYGv6",
        "outputId": "4daaafd1-7d90-46cc-9df1-61b060b650f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "XUDJ9dnHYGv6",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling XLA function... (for the second time -- will be fast)\n",
            "CPU times: user 5.42 ms, sys: 50 µs, total: 5.47 ms\n",
            "Wall time: 4.45 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1,), dtype=int64, numpy=array([3711])>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delving into the details...\n",
        "\n",
        "# Note: execution times are deeply dependent on hardware -- a 3090 was used here.\n",
        "import tensorflow as tf\n",
        "\n",
        "@tf.function(jit_compile=True)\n",
        "def max_plus_constant(tensor, scalar):\n",
        "    return tf.math.reduce_max(tensor) + scalar"
      ],
      "metadata": {
        "id": "cDnbjiTaYMEO"
      },
      "id": "cDnbjiTaYMEO",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Slow: XLA compilation will kick in, as it is the first call\n",
        "max_plus_constant(tf.constant([0, 0, 0]), 1)"
      ],
      "metadata": {
        "id": "-SZ2ugruYgqZ",
        "outputId": "0c8cfe90-9d70-46ac-ede5-70db801d8016",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "-SZ2ugruYgqZ",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 44.6 ms, sys: 3 µs, total: 44.6 ms\n",
            "Wall time: 101 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int32, numpy=1>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Fast: Not the first call with this tensor shape, tensor type, and exact same\n",
        "# non-tensor argument\n",
        "max_plus_constant(tf.constant([1000, 0, -10]), 1)"
      ],
      "metadata": {
        "id": "JG_mGihtYdb8",
        "outputId": "47d3cada-9dbe-4ae6-ee9b-ee3decc1cb54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "JG_mGihtYdb8",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.46 ms, sys: 64 µs, total: 1.52 ms\n",
            "Wall time: 1.07 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int32, numpy=1001>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Slow: Different tensor type\n",
        "max_plus_constant(tf.constant([0, 0, 0], dtype=tf.int64), 1)"
      ],
      "metadata": {
        "id": "VoZ2GpOJYnIp",
        "outputId": "dda33264-a72c-44bc-fd1c-3c2ecca622d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "VoZ2GpOJYnIp",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 19.2 ms, sys: 0 ns, total: 19.2 ms\n",
            "Wall time: 75.6 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int64, numpy=1>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Slow: Different tensor shape\n",
        "max_plus_constant(tf.constant([0, 0, 0, 0]), 1)"
      ],
      "metadata": {
        "id": "3YAtjrACYq4-",
        "outputId": "b4b3e06d-2232-41de-b94e-f79e32ba6619",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3YAtjrACYq4-",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 17 ms, sys: 2.14 ms, total: 19.2 ms\n",
            "Wall time: 74.5 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int32, numpy=1>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Slow: Different non-tensor argument\n",
        "max_plus_constant(tf.constant([0, 0, 0]), 2)"
      ],
      "metadata": {
        "id": "bsQ2N3NIYus2",
        "outputId": "bbeb3c30-84bd-47e6-eee3-e983ef79016f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "bsQ2N3NIYus2",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 18.6 ms, sys: 1.06 ms, total: 19.6 ms\n",
            "Wall time: 75.9 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int32, numpy=2>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.eos_token_id, tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "dSfGEo6fotYu",
        "outputId": "3b72b642-250d-432d-8655-add386bd93da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "dSfGEo6fotYu",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50256, 50256)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.pad_token_id = model.config.eos_token_id"
      ],
      "metadata": {
        "id": "5MOYy7WBzGVO"
      },
      "id": "5MOYy7WBzGVO",
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.pad_token_id, model.config.eos_token_id"
      ],
      "metadata": {
        "id": "Vv7AtMoKziAI",
        "outputId": "c17eb4d7-a5b3-45a1-b030-bfc978fe6c5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Vv7AtMoKziAI",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50256, 50256)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_1 = [\"TensorFlow is\"]\n",
        "input_2 = [\"TensorFlow is a\"]\n",
        "\n",
        "# One line to create a XLA generation function\n",
        "xla_generate = tf.function(model.generate, jit_compile=True)\n",
        "\n",
        "# Calls XLA generation without padding\n",
        "tokenized_input_1 = tokenizer(input_1, return_tensors=\"tf\")  # length = 4\n",
        "tokenized_input_2 = tokenizer(input_2, return_tensors=\"tf\")  # length = 5\n",
        "print(f\"`tokenized_input_1` shape = {tokenized_input_1.input_ids.shape}\")\n",
        "print(f\"`tokenized_input_2` shape = {tokenized_input_2.input_ids.shape}\")"
      ],
      "metadata": {
        "id": "TMaBWTkths9p",
        "outputId": "77bc0fef-ed48-489f-b0ba-328da32782db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "TMaBWTkths9p",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`tokenized_input_1` shape = (1, 4)\n",
            "`tokenized_input_2` shape = (1, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated = model.generate(**tokenized_input_1, max_new_tokens=50)\n",
        "print(\"Sampling output: \", tokenizer.decode(generated[0]))"
      ],
      "metadata": {
        "id": "2rasse1yseoD",
        "outputId": "c2ce779f-f91a-450b-b0f2-15909b53b197",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2rasse1yseoD",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampling output:  TensorFlow is a very powerful and flexible programming language that can be used to build complex neural networks.\n",
            "\n",
            "The goal of this article is to provide a simple and easy way to build a neural network using the Python programming language.\n",
            "\n",
            "The goal of this article\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Calling XLA generation with tokenized_input_1...\")\n",
        "print(\"(will be slow as it is the first call)\")\n",
        "start = time.time_ns()\n",
        "xla_generate(**tokenized_input_1, max_new_tokens=50)\n",
        "end = time.time_ns()\n",
        "print(f\"Execution time -- {(end - start) / 1e6:.1f} ms\\n\")"
      ],
      "metadata": {
        "id": "dEXO9J_ynR71",
        "outputId": "56ab8aa5-cc84-44a4-a61e-890099449741",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        }
      },
      "id": "dEXO9J_ynR71",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling XLA generation with tokenized_input_1...\n",
            "(will be slow as it is the first call)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7ffb44b4eee0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7ffb44b4eee0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-2953c9b2e2dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"(will be slow as it is the first call)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mxla_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtokenized_input_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Execution time -- {(end - start) / 1e6:.1f} ms\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation/tf_utils.py\u001b[0m in \u001b[0;36mtf__generate\u001b[0;34m(self, input_ids, generation_config, seed, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m                     \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_contrastive_search_gen_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body_34\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body_34\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_34\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_34\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'do_return'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_kwargs['attention_mask']\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_kwargs['encoder_outputs']['last_hidden_state']\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'retval_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_kwargs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                 \u001b[0mlogits_warper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logits_warper'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_greedy_gen_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body_35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body_35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'do_return'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_kwargs['attention_mask']\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_kwargs['encoder_outputs']['last_hidden_state']\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'retval_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_kwargs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf__generate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation/tf_utils.py\u001b[0m in \u001b[0;36mif_body_35\u001b[0;34m()\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m                         \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m                         \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgreedy_search\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meos_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m...\n\u001b[0m\u001b[1;32m    376\u001b[0m                     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m                         \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation/tf_utils.py\u001b[0m in \u001b[0;36mtf__greedy_search\u001b[0;34m(self, input_ids, max_length, pad_token_id, eos_token_id, logits_processor, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m                             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mfscope_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval__2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_return_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinished_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgreedy_search_body_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinished_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mget_state_12\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation/tf_utils.py\u001b[0m in \u001b[0;36mgreedy_search_body_fn\u001b[0;34m(generated, finished_sequences, cur_len, model_kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m                                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                             \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'past_key_values'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope_2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body_10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body_10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                         \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body_11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body_11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'model_kwargs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                             \u001b[0mdo_return_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation/tf_utils.py\u001b[0m in \u001b[0;36mif_body_11\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m                         \u001b[0;32mdef\u001b[0m \u001b[0mif_body_11\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                             \u001b[0;32mnonlocal\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                             \u001b[0mmodel_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_model_kwargs_for_xla_generation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_encoder_decoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_batch_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                         \u001b[0;32mdef\u001b[0m \u001b[0melse_body_11\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation/tf_utils.py\u001b[0m in \u001b[0;36mtf___update_model_kwargs_for_xla_generation\u001b[0;34m(self, model_outputs, model_kwargs, cur_len, max_length, batch_size, is_encoder_decoder, batch_axis)\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0melse_body_4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m                 \u001b[0mis_past_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'past_key_values'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation/tf_utils.py\u001b[0m in \u001b[0;36mif_body_4\u001b[0;34m()\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mif_body_4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf'No known `past_key_values variable` found in model outputs (model outputs keys: {ag__.converted_call(ag__.ld(list), (ag__.converted_call(ag__.ld(model_outputs).keys, (), None, fscope),), None, fscope)})'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0melse_body_4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/transformers/generation/tf_utils.py\", line 768, in generate  *\n        input_ids,\n    File \"/usr/local/lib/python3.8/dist-packages/transformers/generation/tf_utils.py\", line 1424, in greedy_search_body_fn  *\n        model_kwargs = self._update_model_kwargs_for_xla_generation(\n    File \"/usr/local/lib/python3.8/dist-packages/transformers/generation/tf_utils.py\", line 1143, in _update_model_kwargs_for_xla_generation  *\n        raise ValueError(\n\n    ValueError: No known `past_key_values variable` found in model outputs (model outputs keys: ['logits'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Calling XLA generation with tokenized_input_2...\")\n",
        "print(\"(has a different length = will trigger tracing again)\")\n",
        "start = time.time_ns()\n",
        "xla_generate(**tokenized_input_2)\n",
        "end = time.time_ns()\n",
        "print(f\"Execution time -- {(end - start) / 1e6:.1f} ms\\n\")"
      ],
      "metadata": {
        "id": "ya4NXR2DoBPR",
        "outputId": "fb96bec1-2151-4401-a1ab-fb8909af4c75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ya4NXR2DoBPR",
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling XLA generation with tokenized_input_2...\n",
            "(has a different length = will trigger tracing again)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:371: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  return py_builtins.overload_of(f)(*args)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution time -- 13855.5 ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ro4HEc_d52Fn"
      },
      "id": "Ro4HEc_d52Fn",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e43bcc64a96f44a0bb561c0d95a32d42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e80ef4334a134e0ebf6290a63d292032",
              "IPY_MODEL_b6fb76adb08e4627a50cf05692d984a5",
              "IPY_MODEL_847e84f7ea414eb188e7f7f3c39d6da0"
            ],
            "layout": "IPY_MODEL_8b1bb191b50d451da2d0dc8214fea25b"
          }
        },
        "e80ef4334a134e0ebf6290a63d292032": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24a330a54c7b4be59eb958d202c72a73",
            "placeholder": "​",
            "style": "IPY_MODEL_1d12658cbc024ab8aca97946ae7f3d51",
            "value": "100%"
          }
        },
        "b6fb76adb08e4627a50cf05692d984a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dce508846ac143f7a8b65629f60dc0f5",
            "max": 12,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d580a52c476f4c92871087559e268dc7",
            "value": 12
          }
        },
        "847e84f7ea414eb188e7f7f3c39d6da0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d55443d43cf44113a85d53106e9ea13f",
            "placeholder": "​",
            "style": "IPY_MODEL_c18aa6d09d234533b9fff6fc6d7a0a05",
            "value": " 12/12 [00:03&lt;00:00,  3.33ba/s]"
          }
        },
        "8b1bb191b50d451da2d0dc8214fea25b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24a330a54c7b4be59eb958d202c72a73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d12658cbc024ab8aca97946ae7f3d51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dce508846ac143f7a8b65629f60dc0f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d580a52c476f4c92871087559e268dc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d55443d43cf44113a85d53106e9ea13f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c18aa6d09d234533b9fff6fc6d7a0a05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}