{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "732829f7-dacb-4dbe-9478-68c10eb5e49c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "732829f7-dacb-4dbe-9478-68c10eb5e49c",
        "outputId": "205ed70d-16ae-4512-bfb0-7cb0e0ad736a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformers                  4.26.0.dev0\n"
          ]
        }
      ],
      "source": [
        "!pip list | grep transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "43bab354-e254-44ac-8793-999f4498b7d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43bab354-e254-44ac-8793-999f4498b7d4",
        "outputId": "8a09449c-9887-42f3-ab82-df49ccfa8603"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasets                      2.8.0\n",
            "tensorflow-datasets           4.8.1\n",
            "vega-datasets                 0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip list | grep dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upgrade to latest version from main as per\n",
        "# https://github.com/huggingface/transformers/issues/20750\n",
        "# to avoid\n",
        "# AttributeError: module 'keras.engine.data_adapter' has no attribute 'expand_1d'\n",
        "# when fitting\n",
        "!pip install --upgrade git+https://github.com/huggingface/transformers.git\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "3fP9Xh-C6Thi"
      },
      "id": "3fP9Xh-C6Thi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "426ffdb1-55d1-4275-a986-52494f557287",
      "metadata": {
        "id": "426ffdb1-55d1-4275-a986-52494f557287"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "dc3b28be-8123-4337-824a-b295968e7403",
      "metadata": {
        "id": "dc3b28be-8123-4337-824a-b295968e7403"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# from transformers import TFGPT2LMHeadModel\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel, pipeline, AutoTokenizer, TFAutoModelForCausalLM\n",
        "from datasets import Dataset, load_dataset\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d360b762-a0bc-425e-bba4-a3ae612d54b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d360b762-a0bc-425e-bba4-a3ae612d54b1",
        "outputId": "48ed36b0-edbb-4c15-bf00-8b31274f8402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REPLICAS:  1\n"
          ]
        }
      ],
      "source": [
        "# Try to run on TPU if available\n",
        "# Detect hardware, return appropriate distribution strategy\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    print(\"Running on TPU \", tpu.cluster_spec().as_dict()[\"worker\"])\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS = 128\n",
        "\n",
        "EOS_TOKEN = \"<|endoftext|>\"\n",
        "PAD_TOKEN = \"<|pad|>\""
      ],
      "metadata": {
        "id": "2jGce1GJNh68"
      },
      "id": "2jGce1GJNh68",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "965ad243-2895-4b85-b913-589b8e87ce7c",
      "metadata": {
        "id": "965ad243-2895-4b85-b913-589b8e87ce7c"
      },
      "outputs": [],
      "source": [
        "# ORIGINAL\n",
        "\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained(\n",
        "#     'gpt2',\n",
        "#     eos_token=EOS_TOKEN,\n",
        "#     pad_token=PAD_TOKEN,\n",
        "#     max_length=MAX_TOKENS,\n",
        "#     padding_side='left'\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    'gpt2',\n",
        "    eos_token=EOS_TOKEN,\n",
        "    pad_token=PAD_TOKEN,\n",
        "    max_length=MAX_TOKENS,\n",
        "    padding_side='left'\n",
        ")\n",
        "\n",
        "# tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOT6QLo5OGlQ",
        "outputId": "81b98e2c-c115-4707-eb25-395655ca7cae"
      },
      "id": "dOT6QLo5OGlQ",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f1549414-31ce-4a9f-84b5-c13355669c71",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1549414-31ce-4a9f-84b5-c13355669c71",
        "outputId": "549cad65-adb3-41d8-9c6f-886df09b49a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-7d45206e1c87ab3a\n",
            "/usr/local/lib/python3.8/dist-packages/datasets/builder.py:712: FutureWarning: 'use_auth_token' was deprecated in version 2.7.1 and will be removed in 3.0.0. Pass `use_auth_token` to the initializer/`load_dataset_builder` instead.\n",
            "  warnings.warn(\n",
            "WARNING:datasets.builder:Found cached dataset text (/root/.cache/huggingface/datasets/text/default-7d45206e1c87ab3a/0.0.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' Living on Mars , Deckard is acting as a consultant to a movie crew filming the story of his Blade Runner days . He finds himself drawn into a mission on behalf of the replicants he was once assigned to kill . Meanwhile , the mystery surrounding the beginnings of the Tyrell Corporation is being dragged out into the light . ',\n",
              " ' Beginning several months after the events in Blade Runner , Deckard has retired to an isolated shack outside the city , taking the replicant Rachael with him in a Tyrell transport container , which slows down the replicant aging process . He is approached by a woman who explains she is Sarah Tyrell , niece of Eldon Tyrell , heiress to the entire Tyrell Corporation and the human template templant for the Rachael replicant . She asks Deckard to hunt down the missing sixth replicant . At the same time , the human template for Roy Batty hires Dave Holden , the blade runner attacked by Leon , to help him hunt down the man he believes is the sixth replicant Deckard . Deckard and Holden s investigations lead them to re visit Sebastian , Bryant , and John Isidore from the book Do Androids Dream Of Electric Sheep ? , learning more about the nature of the blade runners and the replicants . When Deckard , Batty , and Holden finally clash , Batty s inhuman fighting prowess leads Holden to believe he has been duped all along and that Batty is the sixth replicant he shoots him . Deckard returns to Sarah with his suspicion there is no sixth replicant . Sarah , speaking via a remote camera , confesses that she created and maintained the rumor herself , to deliberately discredit and eventually destroy the Tyrell Corporation , after her uncle Eldon created Rachael based on her and then abandoned the real Sarah . Sarah brings Rachael back to the Corporation building to meet with Deckard , and he escapes with her . However , Holden recovering from his injuries during the fight later finds the truth Rachael has been killed by Tyrell agents , and the Rachael who escaped with Deckard was actually Sarah . She has completed her revenge by both destroying Tyrell , and taking back Rachael s place . ']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "ds = Dataset.from_text('booksummaries_cleaned2.txt')# 'testdata2.txt') # 'text100.txt') # 'booksummaries_cleaned2.txt')\n",
        "ds['text'][:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d3bcb95-bbff-462a-9dba-242d4bf27116",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "8d3bcb95-bbff-462a-9dba-242d4bf27116"
      },
      "source": [
        "## Explanation of working of tokenize_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "86c09e6a-19de-416a-b99e-952a216894cb",
      "metadata": {
        "id": "86c09e6a-19de-416a-b99e-952a216894cb"
      },
      "outputs": [],
      "source": [
        "# examples = [BOS_TOKEN + ex + EOS_TOKEN for ex in ds[\"text\"]]\n",
        "# examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9d20f85b-f042-4077-ad3c-55f6981feff3",
      "metadata": {
        "id": "9d20f85b-f042-4077-ad3c-55f6981feff3"
      },
      "outputs": [],
      "source": [
        "# output = tokenizer(\n",
        "#     examples,\n",
        "#     add_special_tokens=True,  # Only adds pad not eos and bos\n",
        "#     max_length=MAX_TOKENS,\n",
        "#     truncation=True,\n",
        "#     padding='max_length',\n",
        "#     # return_tensors='tf'\n",
        "# )\n",
        "# output[\"input_ids\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3024dd33-a83f-4293-b1ca-05082641bc92",
      "metadata": {
        "id": "3024dd33-a83f-4293-b1ca-05082641bc92"
      },
      "outputs": [],
      "source": [
        "# # Drop the first token\n",
        "# output[\"labels\"] = [x[1:] for x in output[\"input_ids\"]]\n",
        "# pprint(output[\"labels\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "384bb5a1-c86d-4d9a-814c-9c406cfc9cde",
      "metadata": {
        "id": "384bb5a1-c86d-4d9a-814c-9c406cfc9cde"
      },
      "outputs": [],
      "source": [
        "# pprint(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ebaa495d-0233-47f6-8bfa-8565e111b8a3",
      "metadata": {
        "id": "ebaa495d-0233-47f6-8bfa-8565e111b8a3"
      },
      "outputs": [],
      "source": [
        "# # Replace all occurences of PAD_TOKEN with -100\n",
        "# output[\"labels\"] = [\n",
        "#     [-100 if x == tokenizer.pad_token_id else x for x in y] for y in output[\"labels\"]\n",
        "# ]\n",
        "# pprint(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f2d207f5-e098-4c18-a2b3-61721ba4f88f",
      "metadata": {
        "id": "f2d207f5-e098-4c18-a2b3-61721ba4f88f"
      },
      "outputs": [],
      "source": [
        "# # truncate input ids and attention mask to account for label shift\n",
        "# output[\"input_ids\"] = [x[:-1] for x in output[\"input_ids\"]]\n",
        "# output[\"attention_mask\"] = [x[:-1] for x in output[\"attention_mask\"]]\n",
        "# pprint(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25c06772-572d-44a9-b34a-2d09e659f116",
      "metadata": {
        "id": "25c06772-572d-44a9-b34a-2d09e659f116"
      },
      "source": [
        "## tokenize_function definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "db6a8808-8199-405f-bd4f-e499fa48a112",
      "metadata": {
        "id": "db6a8808-8199-405f-bd4f-e499fa48a112"
      },
      "outputs": [],
      "source": [
        "# Consolidate into a function\n",
        "\n",
        "def tokenize_function(examples, tokenizer=tokenizer):\n",
        "\n",
        "    examples = [ex + EOS_TOKEN for ex in examples[\"text\"]]\n",
        "\n",
        "    output = tokenizer(\n",
        "        examples,\n",
        "        add_special_tokens=True,  # Only adds pad not eos and bos\n",
        "        max_length=MAX_TOKENS,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        # return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    # Drop the first token\n",
        "    output[\"labels\"] = [x[1:] for x in output[\"input_ids\"]]\n",
        "\n",
        "    # Replace all occurences of PAD_TOKEN with -100\n",
        "    output[\"labels\"] = [\n",
        "        [-100 if x == tokenizer.pad_token_id else x for x in y] for y in output[\"labels\"]\n",
        "    ]\n",
        "\n",
        "    # truncate input ids and attention mask to account for label shift\n",
        "    output[\"input_ids\"] = [x[:-1] for x in output[\"input_ids\"]]\n",
        "    output[\"attention_mask\"] = [x[:-1] for x in output[\"attention_mask\"]]\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d38759fa-c138-4b34-9812-6f1f8ac3cb88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122,
          "referenced_widgets": [
            "486177c999d548929e09ebd60b682467",
            "5f5ca818adf140d8856d2fbda5c60f06",
            "0122d89c2acf4569a4909d4006fc2e14",
            "31f3d1d12a904bd194f627e15adf3b50",
            "3bcc5aa7d08f42a487070a686c40110f",
            "a74216f9fa7a47d99be0c5ee444bb086",
            "a8638d3f920349bfb05b98fe3f51dad2",
            "5c02fd8e8bcb431787297f8668316140",
            "81e148cc7a1040cea215ca2302f6e03c",
            "193a2021ac3d42598c02991c46a3e52c",
            "9667ada71f4648cda82831d417332b03"
          ]
        },
        "id": "d38759fa-c138-4b34-9812-6f1f8ac3cb88",
        "outputId": "a272ce90-d307-4fa2-ed21-223849223328"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/12 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "486177c999d548929e09ebd60b682467"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 11783\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "ds_tokenized = ds.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\"],\n",
        "    load_from_cache_file=False,\n",
        ")\n",
        "ds_tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9c6c7cd3-1498-4250-a7ad-d84620a13e19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c6c7cd3-1498-4250-a7ad-d84620a13e19",
        "outputId": "5c6c8647-07fa-444c-ece8-3f6ccad326d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 9426\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 2357\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "ds_tokenized.set_format(type=\"python\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "ds_tokenized = ds_tokenized.train_test_split(\n",
        "    test_size=0.20, shuffle=True, seed=1, load_from_cache_file=True\n",
        ")\n",
        "ds_tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "564c024e-357b-4605-a0fc-b4cd7de21b69",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "564c024e-357b-4605-a0fc-b4cd7de21b69",
        "outputId": "ab3555fc-bd66-4b9e-f66c-9b1706654af1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.72 s, sys: 438 ms, total: 4.15 s\n",
            "Wall time: 4.14 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# prepare for use in tensorflow\n",
        "train_tensor_inputs = tf.convert_to_tensor(ds_tokenized[\"train\"][\"input_ids\"])\n",
        "train_tensor_labels = tf.convert_to_tensor(ds_tokenized[\"train\"][\"labels\"])\n",
        "train_tensor_mask = tf.convert_to_tensor(ds_tokenized[\"train\"][\"attention_mask\"])\n",
        "train = tf.data.Dataset.from_tensor_slices(\n",
        "    (\n",
        "        {\"input_ids\": train_tensor_inputs, \"attention_mask\": train_tensor_mask},\n",
        "        # {\"labels\": train_tensor_labels},\n",
        "        train_tensor_labels,\n",
        "    )\n",
        ")\n",
        "\n",
        "test_tensor_inputs = tf.convert_to_tensor(ds_tokenized[\"test\"][\"input_ids\"])\n",
        "test_tensor_labels = tf.convert_to_tensor(ds_tokenized[\"test\"][\"labels\"])\n",
        "test_tensor_mask = tf.convert_to_tensor(ds_tokenized[\"test\"][\"attention_mask\"])\n",
        "test = tf.data.Dataset.from_tensor_slices(\n",
        "    (\n",
        "        {\"input_ids\": test_tensor_inputs, \"attention_mask\": test_tensor_mask},\n",
        "        # {\"labels\": test_tensor_labels},\n",
        "        test_tensor_labels,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert to a TF object using new method\n",
        "\n",
        "``` python\n",
        "ds_tf = ds_tokenized.to_tf_dataset(\n",
        "    columns=[\"input_ids\",\"attention_mask\"],\n",
        "    label_cols=[\"labels\"],\n",
        "    batch_size=1,\n",
        "    shuffle=True\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "V9nLB05Lmpx9"
      },
      "id": "V9nLB05Lmpx9"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "fbdb882f-b578-4561-93f7-85276cf0ac86",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbdb882f-b578-4561-93f7-85276cf0ac86",
        "outputId": "2f8351d1-a219-4023-ed03-d5ef87a74ad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BATCH_SIZE:  28\n",
            "len(tokenizer)  50258\n"
          ]
        }
      ],
      "source": [
        "# Model params\n",
        "BATCH_SIZE_PER_REPLICA = 28\n",
        "EPOCHS = 10\n",
        "INITAL_LEARNING_RATE = 0.001\n",
        "\n",
        "try:\n",
        "    BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
        "except NameError as e:\n",
        "    BATCH_SIZE = BATCH_SIZE_PER_REPLICA\n",
        "BUFFER_SIZE = len(train)\n",
        "\n",
        "print(\"BATCH_SIZE: \", BATCH_SIZE)\n",
        "\n",
        "print(\"len(tokenizer) \", len(tokenizer))\n",
        "\n",
        "# prepare data for consumption\n",
        "train_ds = (\n",
        "    train.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        ")\n",
        "test_ds = test.batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "af84fd8d-6b7c-4106-bce8-63c5b82acab3",
      "metadata": {
        "id": "af84fd8d-6b7c-4106-bce8-63c5b82acab3"
      },
      "outputs": [],
      "source": [
        "# Decreasing learning rate scheduler\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    INITAL_LEARNING_RATE,\n",
        "    decay_steps=500,\n",
        "    decay_rate=0.7,\n",
        "    staircase=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ORIGINAL\n",
        "# initialize model, use_cache=False important! else wrong shape at loss calc\n",
        "with strategy.scope():\n",
        "    model = TFAutoModelForCausalLM.from_pretrained(\n",
        "        'gpt2',\n",
        "        # use_cache=False,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "    model.compile(optimizer=optimizer, loss=model.hf_compute_loss)\n",
        "    model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXQ2YU6cO8eh",
        "outputId": "6011bb1f-6bd3-4398-cbcd-27f7ec570b73"
      },
      "id": "NXQ2YU6cO8eh",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tfgpt2lm_head_model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " transformer (TFGPT2MainLaye  multiple                 124440576 \n",
            " r)                                                              \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 124,440,576\n",
            "Trainable params: 124,440,576\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # PROBLEM IDENTIFIED: Don't set use_cache=False\n",
        "# model = TFAutoModelForCausalLM.from_pretrained(\n",
        "#         'gpt2',\n",
        "#         # use_cache=False,\n",
        "#         pad_token_id=tokenizer.pad_token_id,\n",
        "#         eos_token_id=tokenizer.eos_token_id,\n",
        "#     )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDZ-uBitbvpp",
        "outputId": "08a8f92e-36be-41ce-fc7e-24dc3a1a9772"
      },
      "id": "oDZ-uBitbvpp",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # initialize model, use_cache=False important! else wrong shape at loss calc\n",
        "# with strategy.scope():\n",
        "#     model = TFAutoModelForCausalLM.from_pretrained(\n",
        "#         'gpt2',\n",
        "#         use_cache=False,\n",
        "#         pad_token_id=tokenizer.eos_token_id, # pad_token_id,\n",
        "#         eos_token_id=tokenizer.eos_token_id,\n",
        "#     )\n",
        "#     # model.resize_token_embeddings(len(tokenizer))\n",
        "#     # optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "#     # model.compile(optimizer=optimizer, loss=model.hf_compute_loss)\n",
        "#     model.summary()"
      ],
      "metadata": {
        "id": "AbmFiJrQO46_"
      },
      "id": "AbmFiJrQO46_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1bce2c81-e38e-4765-8e41-ba97b5f2b0c4",
      "metadata": {
        "id": "1bce2c81-e38e-4765-8e41-ba97b5f2b0c4"
      },
      "outputs": [],
      "source": [
        "now = datetime.now().strftime(\"%Y-%m-%d_%H%M\")\n",
        "\n",
        "# # Fix TPU save model issue?\n",
        "# save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost') \n",
        "\n",
        "# Create callbacks\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping( \n",
        "        monitor=\"val_loss\", verbose=1, patience=1, restore_best_weights=True\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        \"models/\" + now + \"_GPT2-Model_{epoch:02d}_{val_loss:.4f}\",\n",
        "        monitor=\"val_loss\",\n",
        "        save_best_only=True,\n",
        "        verbose=1,\n",
        "        # options=save_locally, # Fix TPU save model issue?\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ff48b183-b8d8-41da-9a2f-7bf3d90b68c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff48b183-b8d8-41da-9a2f-7bf3d90b68c9",
        "outputId": "f76c6996-2959-47c2-d3f9-3521a954c724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Params:\n",
            "batch_size: 28\n",
            "Epochs: 10\n",
            "Step p. Epoch: 336\n",
            "Initial Learning rate: 0.001\n",
            "-------------------------------------------------------------\n",
            "Epoch 1/10\n",
            "336/336 [==============================] - ETA: 0s - loss: 4.5344\n",
            "Epoch 1: val_loss improved from inf to 3.88040, saving model to models/2023-01-18_1531_GPT2-Model_01_3.8804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as wte_layer_call_fn, wte_layer_call_and_return_conditional_losses, dropout_74_layer_call_fn, dropout_74_layer_call_and_return_conditional_losses, ln_f_layer_call_fn while saving (showing 5 of 294). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r336/336 [==============================] - 88s 224ms/step - loss: 4.5344 - val_loss: 3.8804\n",
            "Epoch 2/10\n",
            "336/336 [==============================] - ETA: 0s - loss: 3.5754\n",
            "Epoch 2: val_loss improved from 3.88040 to 3.86444, saving model to models/2023-01-18_1531_GPT2-Model_02_3.8644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as wte_layer_call_fn, wte_layer_call_and_return_conditional_losses, dropout_74_layer_call_fn, dropout_74_layer_call_and_return_conditional_losses, ln_f_layer_call_fn while saving (showing 5 of 294). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r336/336 [==============================] - 71s 211ms/step - loss: 3.5754 - val_loss: 3.8644\n",
            "Epoch 3/10\n",
            "336/336 [==============================] - ETA: 0s - loss: 3.1174Restoring model weights from the end of the best epoch: 2.\n",
            "\n",
            "Epoch 3: val_loss did not improve from 3.86444\n",
            "336/336 [==============================] - 39s 116ms/step - loss: 3.1174 - val_loss: 4.0005\n",
            "Epoch 3: early stopping\n"
          ]
        }
      ],
      "source": [
        "# %%time    \n",
        "# Train Model\n",
        "steps_per_epoch = int(BUFFER_SIZE // BATCH_SIZE)\n",
        "print(\n",
        "    f\"Model Params:\\nbatch_size: {BATCH_SIZE}\\nEpochs: {EPOCHS}\\n\"\n",
        "    f\"Step p. Epoch: {steps_per_epoch}\\n\"\n",
        "    f\"Initial Learning rate: {INITAL_LEARNING_RATE}\"\n",
        "    \"\\n-------------------------------------------------------------\"\n",
        ")\n",
        "\n",
        "hist = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=test_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "3eb29058-358f-48b2-8254-cea6a3f0fc35",
      "metadata": {
        "id": "3eb29058-358f-48b2-8254-cea6a3f0fc35"
      },
      "outputs": [],
      "source": [
        "# path_to_model = '/content/models/2023-01-18_0754_GPT2-Model_02_3.9008'\n",
        "# # load model in tpu using Tensorflow's \"SavedModel\" format\n",
        "# with strategy.scope():\n",
        "#     # load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
        "#     restored_model = tf.keras.models.load_model(\n",
        "#         path_to_model,\n",
        "#         # options=load_locally,\n",
        "#         custom_objects={\"hf_compute_loss\": model.hf_compute_loss} # Important!\n",
        "#         )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# new_pipeline = pipeline(\n",
        "#             \"text-generation\",\n",
        "#             model=restored_model,\n",
        "#             tokenizer=tokenizer\n",
        "#         )"
      ],
      "metadata": {
        "id": "rj6CTl_HyL8_"
      },
      "id": "rj6CTl_HyL8_",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # We need to specify max_new_tokens here instead of max_length\n",
        "# new_pipeline('hello', max_new_tokens=10)"
      ],
      "metadata": {
        "id": "UhMD64ORv2gp"
      },
      "id": "UhMD64ORv2gp",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # HF save method\n",
        "# # Note: not using options=save_locally as per TPU fix\n",
        "# # implemented in callback & tf.keras.models.load_model()\n",
        "\n",
        "# model.save_pretrained('/content/models/hf', saved_model=True)\n"
      ],
      "metadata": {
        "id": "QkpoL_t-0If-"
      },
      "id": "QkpoL_t-0If-",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # HF load method\n",
        "# restored_model = TFGPT2LMHeadModel.from_pretrained('/content/models/hf')"
      ],
      "metadata": {
        "id": "q6WKAgzW2Vc7"
      },
      "id": "q6WKAgzW2Vc7",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_pipeline = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer\n",
        "        )"
      ],
      "metadata": {
        "id": "PEHXOqga9-gh"
      },
      "id": "PEHXOqga9-gh",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_pipeline('hello')#, max_new_tokens=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ye_dRZ25-C3r",
        "outputId": "2f424335-79b3-4db3-ef40-34a8feb85cf2"
      },
      "id": "ye_dRZ25-C3r",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/tf_utils.py:603: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/tf_utils.py:702: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 50 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'hello, a married man who lives in the city and falls in love with her for what she does. He is fascinated with the romance and begins fantasizing about the woman he meets his dreams, and eventually realizes that he has got her too. He'}]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing XLA\n",
        "\n",
        "Using these notes: https://huggingface.co/blog/tf-xla-generate"
      ],
      "metadata": {
        "id": "qFbH5HOIY26u"
      },
      "id": "qFbH5HOIY26u"
    },
    {
      "cell_type": "code",
      "source": [
        "# This is how we use the generate function directly, without using\n",
        "# a pipeline\n",
        "\n",
        "inputs = tokenizer([\"I went to the moon and \"], return_tensors=\"tf\")\n",
        "print(inputs)\n",
        "# generated = model.generate(**inputs, max_new_tokens=50)\n",
        "generated = model.generate(**inputs, do_sample=True, max_new_tokens=50)\n",
        "print(\"Sampling output: \", tokenizer.decode(generated[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hNOlnWU-E2L",
        "outputId": "6f92e2c6-eaf0-4f10-bdf8-4b1c41818f13"
      },
      "id": "3hNOlnWU-E2L",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[  40, 1816,  284,  262, 8824,  290,  220]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/tf_utils.py:603: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampling output:  I went to the moon and  saw that my body was on fire and so I put my clothes and shoes back on. I couldn't think for a second on the next thing I went, I really couldn't remember if it was really my baby face or my baby figure.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a test function\n",
        "def most_likely_next_token(inputs):\n",
        "    model_output = model(inputs)\n",
        "    return tf.argmax(model_output.logits[:, -1, :], axis=-1)"
      ],
      "metadata": {
        "id": "LVtCjRBPXYSx"
      },
      "id": "LVtCjRBPXYSx",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print(\"Calling regular function with TensorFlow code...\")\n",
        "most_likely_next_token(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8EyZ4qxZH-A",
        "outputId": "b312b5aa-ec3e-42dc-ca62-ed410a7b0d28"
      },
      "id": "n8EyZ4qxZH-A",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling regular function with TensorFlow code...\n",
            "CPU times: user 134 ms, sys: 5.12 ms, total: 139 ms\n",
            "Wall time: 138 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1,), dtype=int64, numpy=array([3711])>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xla_most_likely_next_token = tf.function(most_likely_next_token, jit_compile=True)"
      ],
      "metadata": {
        "id": "weKdAg2uX5A6"
      },
      "id": "weKdAg2uX5A6",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print(\"Calling XLA function... (for the first time -- will be slow)\")\n",
        "xla_most_likely_next_token(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZuKIHQvYE80",
        "outputId": "f9715483-14d6-46bb-c463-a69bbe1f8ea1"
      },
      "id": "bZuKIHQvYE80",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling XLA function... (for the first time -- will be slow)\n",
            "CPU times: user 5 s, sys: 40.6 ms, total: 5.04 s\n",
            "Wall time: 6.43 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1,), dtype=int64, numpy=array([3711])>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print(\"Calling XLA function... (for the second time -- will be fast)\")\n",
        "xla_most_likely_next_token(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUDJ9dnHYGv6",
        "outputId": "4daaafd1-7d90-46cc-9df1-61b060b650f8"
      },
      "id": "XUDJ9dnHYGv6",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling XLA function... (for the second time -- will be fast)\n",
            "CPU times: user 5.42 ms, sys: 50 µs, total: 5.47 ms\n",
            "Wall time: 4.45 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1,), dtype=int64, numpy=array([3711])>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delving into the details...\n",
        "\n",
        "# Note: execution times are deeply dependent on hardware -- a 3090 was used here.\n",
        "import tensorflow as tf\n",
        "\n",
        "@tf.function(jit_compile=True)\n",
        "def max_plus_constant(tensor, scalar):\n",
        "    return tf.math.reduce_max(tensor) + scalar"
      ],
      "metadata": {
        "id": "cDnbjiTaYMEO"
      },
      "id": "cDnbjiTaYMEO",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Slow: XLA compilation will kick in, as it is the first call\n",
        "max_plus_constant(tf.constant([0, 0, 0]), 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SZ2ugruYgqZ",
        "outputId": "0c8cfe90-9d70-46ac-ede5-70db801d8016"
      },
      "id": "-SZ2ugruYgqZ",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 44.6 ms, sys: 3 µs, total: 44.6 ms\n",
            "Wall time: 101 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int32, numpy=1>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Fast: Not the first call with this tensor shape, tensor type, and exact same\n",
        "# non-tensor argument\n",
        "max_plus_constant(tf.constant([1000, 0, -10]), 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG_mGihtYdb8",
        "outputId": "47d3cada-9dbe-4ae6-ee9b-ee3decc1cb54"
      },
      "id": "JG_mGihtYdb8",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.46 ms, sys: 64 µs, total: 1.52 ms\n",
            "Wall time: 1.07 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int32, numpy=1001>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Slow: Different tensor type\n",
        "max_plus_constant(tf.constant([0, 0, 0], dtype=tf.int64), 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoZ2GpOJYnIp",
        "outputId": "dda33264-a72c-44bc-fd1c-3c2ecca622d9"
      },
      "id": "VoZ2GpOJYnIp",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 19.2 ms, sys: 0 ns, total: 19.2 ms\n",
            "Wall time: 75.6 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int64, numpy=1>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Slow: Different tensor shape\n",
        "max_plus_constant(tf.constant([0, 0, 0, 0]), 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YAtjrACYq4-",
        "outputId": "b4b3e06d-2232-41de-b94e-f79e32ba6619"
      },
      "id": "3YAtjrACYq4-",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 17 ms, sys: 2.14 ms, total: 19.2 ms\n",
            "Wall time: 74.5 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int32, numpy=1>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Slow: Different non-tensor argument\n",
        "max_plus_constant(tf.constant([0, 0, 0]), 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsQ2N3NIYus2",
        "outputId": "bbeb3c30-84bd-47e6-eee3-e983ef79016f"
      },
      "id": "bsQ2N3NIYus2",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 18.6 ms, sys: 1.06 ms, total: 19.6 ms\n",
            "Wall time: 75.9 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int32, numpy=2>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.eos_token_id, tokenizer.pad_token_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSfGEo6fotYu",
        "outputId": "3b72b642-250d-432d-8655-add386bd93da"
      },
      "id": "dSfGEo6fotYu",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50256, 50256)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.pad_token_id = model.config.eos_token_id"
      ],
      "metadata": {
        "id": "5MOYy7WBzGVO"
      },
      "id": "5MOYy7WBzGVO",
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.pad_token_id, model.config.eos_token_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vv7AtMoKziAI",
        "outputId": "c17eb4d7-a5b3-45a1-b030-bfc978fe6c5d"
      },
      "id": "Vv7AtMoKziAI",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50256, 50256)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_1 = [\"TensorFlow is\"]\n",
        "input_2 = [\"TensorFlow is a\"]"
      ],
      "metadata": {
        "id": "TMaBWTkths9p"
      },
      "id": "TMaBWTkths9p",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Calls XLA generation without padding\n",
        "# tokenized_input_1 = tokenizer(input_1, return_tensors=\"tf\")  # length = 4\n",
        "# tokenized_input_2 = tokenizer(input_2, return_tensors=\"tf\")  # length = 5\n",
        "# print(f\"`tokenized_input_1` shape = {tokenized_input_1.input_ids.shape}\")\n",
        "# print(f\"`tokenized_input_2` shape = {tokenized_input_2.input_ids.shape}\")"
      ],
      "metadata": {
        "id": "hDkf8A7Qb99p"
      },
      "id": "hDkf8A7Qb99p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generated = model.generate(**tokenized_input_1, max_new_tokens=50)\n",
        "# print(\"Sampling output: \", tokenizer.decode(generated[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rasse1yseoD",
        "outputId": "c2ce779f-f91a-450b-b0f2-15909b53b197"
      },
      "id": "2rasse1yseoD",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampling output:  TensorFlow is a very powerful and flexible programming language that can be used to build complex neural networks.\n",
            "\n",
            "The goal of this article is to provide a simple and easy way to build a neural network using the Python programming language.\n",
            "\n",
            "The goal of this article\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenization_kwargs = {\"pad_to_multiple_of\": 32, \"padding\": True, \"return_tensors\": \"tf\"}\n",
        "generation_kwargs = {\"num_beams\": 4, \"max_new_tokens\": 500}\n",
        "\n",
        "# One line to create a XLA generation function\n",
        "xla_generate = tf.function(model.generate, jit_compile=True)"
      ],
      "metadata": {
        "id": "Ro4HEc_d52Fn"
      },
      "id": "Ro4HEc_d52Fn",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_prompts = [f\"The best thing about {country} is\" for country in [\"Spain\", \"Japan\", \"Angola\"]]\n",
        "for input_prompt in input_prompts:\n",
        "    tokenized_inputs = tokenizer([input_prompt], **tokenization_kwargs)\n",
        "    start = time.time_ns()\n",
        "    generated_text = xla_generate(**tokenized_inputs, **generation_kwargs)\n",
        "    end = time.time_ns()\n",
        "    decoded_text = tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
        "    print(f\"Original prompt -- {input_prompt}\")\n",
        "    print(f\"Generated -- {decoded_text}\")\n",
        "    print(f\"Execution time -- {(end - start) / 1e6:.1f} ms\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTU-lJsRcgzq",
        "outputId": "d6b51483-1bce-43af-ce72-e9a19dfb9415"
      },
      "id": "NTU-lJsRcgzq",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original prompt -- The best thing about Spain is\n",
            "Generated -- The best thing about Spain is that the people of the country are very proud of their country. The people of the country are very proud of their country. The people of the country are very proud of their country. The people of the country are very proud of their country. The people of the country are very proud of their country. The people of the country are very proud of their country. The people of the country are very proud of their country. The people of the country are very proud of their country. The people of the country are very proud of their country. The people of Spain are very proud of their country. \n",
            "Execution time -- 18149.5 ms\n",
            "\n",
            "Original prompt -- The best thing about Japan is\n",
            "Generated -- The best thing about Japan is that people live in peace. The best thing about Japan is that there is no violence. The best thing about Japan is that there is no violence. The best thing about Japan is that there is no violence. The best thing about Japan is that there is no violence. The best thing about Japan is that there is no violence. The best thing about Japan is that there is no violence. The best thing about Japan is that there is no violence. The best thing about Japan is that there is no violence. The best thing about Japan is that there is no violence. The best thing about Japan is that there is no violence. The best thing about Japan is that there is that there is no violence. The best thing about Japan is that there is that there is that there is no violence. The best thing about Japan is that there is that there is that there is no violence. The best thing about Japan is that is that there is that there is that there is that there is that there is that there is that there is that there is violence is that the violence is not violence is not violence is not violence is not violence is not violence is not violence is not violence is not violence is not violence is not violence is not violence is not violence is not violence is not violence is not violence is not violence is not violence is not violence is not violence is not violence is not violence is not violence is not violence is not violence is not violence is not violence is not violence is not violence is not violence. The best thing about Japan is that is that is that is that is violence is not violence is not violence is not violence is not violence. The best thing about Japan is that is that is that is violence is that is violence is not violence is not violence. The best thing about Japan is that is that is violence is that is violence is that is violence is not violence. The best thing about Japan is that is that is that is violence is that is that is violence is that is that is that is violence is that is that is that is that is that is violence is that is that is that is that is that is that is that is violence is that is that is that is that is that is that is that is that is that is that is that is that there is violence is that is that is that is that is that is that is that is that is that is that is that is that there is violence is not violence is not violence. The best thing about Japan is\n",
            "Execution time -- 1726.2 ms\n",
            "\n",
            "Original prompt -- The best thing about Angola is\n",
            "Generated -- The best thing about Angola is that the people of the country are very good people. The people of the country are very good people. The people of the country are very good people. The people of the country are very good people. The people of the country are very good people. The people of the country are very good people. The people of the country are very good people. The people of the country are very good people. The people of the country are very good people. The people of the country are very good people. The people of Angola are very good people. The people of the country are very good people. \n",
            "Execution time -- 1741.6 ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_prompts = [f\"Let me tell you a story about {country}\" for country in [\"Italy\", \"Ireland\", \"England\"]]\n",
        "for input_prompt in input_prompts:\n",
        "    tokenized_inputs = tokenizer([input_prompt], **tokenization_kwargs)\n",
        "    start = time.time_ns()\n",
        "    generated_text = xla_generate(**tokenized_inputs, **generation_kwargs)\n",
        "    end = time.time_ns()\n",
        "    decoded_text = tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
        "    print(f\"Original prompt -- {input_prompt}\")\n",
        "    print(f\"Generated -- {decoded_text}\")\n",
        "    print(f\"Execution time -- {(end - start) / 1e6:.1f} ms\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuMKS6iTc2Mq",
        "outputId": "24ede509-f9a6-4661-bb8e-ca94a317cc0c"
      },
      "id": "EuMKS6iTc2Mq",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original prompt -- Let me tell you a story about Italy\n",
            "Generated -- Let me tell you a story about Italy, a country that has been shaken by the death of a young man. The story is told from the point of view of the young man s mother, who\n",
            "Execution time -- 90.1 ms\n",
            "\n",
            "Original prompt -- Let me tell you a story about Ireland\n",
            "Generated -- Let me tell you a story about Ireland, a young Irish woman who has just moved to the United States. Her parents are divorced, and she has no children. Her father is a lawyer, and\n",
            "Execution time -- 81.7 ms\n",
            "\n",
            "Original prompt -- Let me tell you a story about England\n",
            "Generated -- Let me tell you a story about England, a country that has been shaken by the death of a young man. The story is told from the point of view of the young man s mother, who\n",
            "Execution time -- 84.9 ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0HVqhldyhdOx"
      },
      "id": "0HVqhldyhdOx",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "486177c999d548929e09ebd60b682467": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f5ca818adf140d8856d2fbda5c60f06",
              "IPY_MODEL_0122d89c2acf4569a4909d4006fc2e14",
              "IPY_MODEL_31f3d1d12a904bd194f627e15adf3b50"
            ],
            "layout": "IPY_MODEL_3bcc5aa7d08f42a487070a686c40110f"
          }
        },
        "5f5ca818adf140d8856d2fbda5c60f06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a74216f9fa7a47d99be0c5ee444bb086",
            "placeholder": "​",
            "style": "IPY_MODEL_a8638d3f920349bfb05b98fe3f51dad2",
            "value": "100%"
          }
        },
        "0122d89c2acf4569a4909d4006fc2e14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c02fd8e8bcb431787297f8668316140",
            "max": 12,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81e148cc7a1040cea215ca2302f6e03c",
            "value": 12
          }
        },
        "31f3d1d12a904bd194f627e15adf3b50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_193a2021ac3d42598c02991c46a3e52c",
            "placeholder": "​",
            "style": "IPY_MODEL_9667ada71f4648cda82831d417332b03",
            "value": " 12/12 [00:03&lt;00:00,  3.91ba/s]"
          }
        },
        "3bcc5aa7d08f42a487070a686c40110f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a74216f9fa7a47d99be0c5ee444bb086": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8638d3f920349bfb05b98fe3f51dad2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c02fd8e8bcb431787297f8668316140": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81e148cc7a1040cea215ca2302f6e03c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "193a2021ac3d42598c02991c46a3e52c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9667ada71f4648cda82831d417332b03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}